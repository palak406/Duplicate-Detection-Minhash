{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#<b> Question 5: Implement the Locality Minhash/LSH algorithm discussed in class, using Spark (Scala, Java, or Python) <br>\n",
    "    <B> NOTE: I have implemented all the code in databricks because of the memory issues in jupyter notebook. and i have attached cell by cell implementation screenshot for your reference if you need the data to be displayed in tabular format.<br>\n",
    " <b>#References (Understod the concept from) <br>\n",
    "     https://databricks.com/blog/2017/05/09/detecting-abuse-scale-locality-sensitive-hashing-uber-engineering.html <br>\n",
    " https://mattilyra.github.io/2017/05/23/document-deduplication-with-lsh.html <br>\n",
    " http://mccormickml.com/2015/06/12/minhash-tutorial-with-python-code/<br>\n",
    "     https://spark.apache.org/docs/1.5.2/api/python/pyspark.ml.html<br>\n",
    "https://spark.apache.org/docs/2.2.3/ml-features.html#n-gram <br>\n",
    "         https://towardsdatascience.com/countvectorizer-hashingtf-e66f169e2d4e<br>\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#creating spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Big Data Q5\")\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.172.225.218:42107\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#importing various ml libraries and pyspark functions necessary to run the code. \n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, NGram\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, NGram\n",
    "import re as regexp\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import MinHashLSH\n",
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Defining a function named change that removes new line character which do not need to be compared\n",
    "def change(pair):\n",
    "    line = regexp.sub(r'\\n\\s*\\n','\\n',pair[1],regexp.MULTILINE)\n",
    "    return [[[name for name in pair[0].split('/')][-1] ,line]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Flattening the data and loading the text files\n",
    "data = sc.wholeTextFiles(\"/FileStore/tables/*.txt\").flatMap(change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Converting the data into dataframes and storing it in data_df\n",
    "data_df = data.toDF(['doc_title','doc_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Using tokenizer function which is the process of taking text (such as a sentence) and breaking it into individual terms (usually words)\n",
    "#The Tokenizer splits the txt file into output columns of doc_words,doc_title and doc_content\n",
    "tokenizer = Tokenizer(inputCol=\"doc_content\",outputCol=\"doc_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Transforming the dataframe using the tokenizer object\n",
    "#Transform method is taken from pyspark.ml.Transformer\n",
    "token = tokenizer.transform(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#selecting various columns\n",
    "data_select = token.select(\"doc_title\",\"doc_content\", \"doc_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+---------+--------------------+--------------------+\n",
       "doc_title|         doc_content|           doc_words|\n",
       "+---------+--------------------+--------------------+\n",
       " amem.txt| \n",
       " The American M...|[, , , the, ameri...|\n",
       " amwh.txt| \r\n",
       "  The American...|[, , , , , the, a...|\n",
       " army.txt| \r\n",
       " Manual For Ar...|[, , , , manual, ...|\n",
       " aunt.txt| \r\n",
       "  &#34;Aunt Babett...|[, , , , , &#34;aunt,...|\n",
       " bart.txt| \r\n",
       " THE \r\n",
       "IDEAL B...|[, , , , the, , ,...|\n",
       " beec.txt| \r\n",
       "    A bookplat...|[, , , , , , , a,...|\n",
       " blue.txt| \r\n",
       " THE BLUE GRAS...|[, , , , the, blu...|\n",
       " bost.txt| \r\n",
       " THE  BOSTON C...|[, , , , the, , b...|\n",
       " brkf.txt| \r\n",
       " Breakfast, Lu...|[, , , , breakfas...|\n",
       " buck.txt| \r\n",
       " Practical  Ho...|[, , , , practica...|\n",
       " cclu.txt| \r\n",
       "   Cooking in ...|[, , , , , , cook...|\n",
       " chas.txt| \r\n",
       " Dr. Chase&#39;s R...|[, , , , dr., cha...|\n",
       " chin.txt| \r\n",
       " Chinese-Japan...|[, , , , chinese-...|\n",
       " choc.txt| \r\n",
       " Chocolate and...|[, , , , chocolat...|\n",
       " comm.txt| \r\n",
       " Common Sense ...|[, , , , common, ...|\n",
       " conf.txt| \r\n",
       " The Complete ...|[, , , , the, com...|\n",
       " coow.txt| \r\n",
       " The Cook&#39;s Ow...|[, , , , the, coo...|\n",
       " creo.txt| \r\n",
       " La Cuisine Cr...|[, , , , la, cuis...|\n",
       " dcvb.txt| \r\n",
       " Directions fo...|[, , , , directio...|\n",
       " dish.txt| \r\n",
       "  Dishes &amp;amp;...|[, , , , , dishes...|\n",
       "+---------+--------------------+--------------------+\n",
       "only showing top 20 rows\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Displaying the result\n",
    "data_select.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Using ngrams function which is a sequence of n tokens (typically words) for some integer n. \n",
    "#The NGram class can be used to transform input features into n-grams.\n",
    "#Going from pair of words, to trigrams, till ngrams combination of words\n",
    "ng = NGram(n=2, inputCol=\"doc_words\", outputCol=\"ngrams\")\n",
    "data_select = ng.transform(data_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#CountVectorizer converts into vectors \n",
    "# fit is used to fit the model \n",
    "#model is transformed \n",
    "count_vect = CountVectorizer(inputCol=\"ngrams\", outputCol=\"features\", vocabSize=100000, minDF=2)\n",
    "model = count_vect.fit(data_select)\n",
    "data_select = model.transform(data_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+---------+--------------------+--------------------+--------------------+--------------------+\n",
       "doc_title|         doc_content|           doc_words|              ngrams|            features|\n",
       "+---------+--------------------+--------------------+--------------------+--------------------+\n",
       " amem.txt| \n",
       " The American M...|[, , , the, ameri...|[ ,  ,  the, the ...|(100000,[0,4,21,2...|\n",
       " amwh.txt| \r\n",
       "  The American...|[, , , , , the, a...|[ ,  ,  ,  ,  the...|(100000,[0,1,2,3,...|\n",
       " army.txt| \r\n",
       " Manual For Ar...|[, , , , manual, ...|[ ,  ,  ,  manual...|(100000,[0,1,2,3,...|\n",
       " aunt.txt| \r\n",
       "  &#34;Aunt Babett...|[, , , , , &#34;aunt,...|[ ,  ,  ,  ,  &#34;au...|(100000,[0,1,2,3,...|\n",
       " bart.txt| \r\n",
       " THE \r\n",
       "IDEAL B...|[, , , , the, , ,...|[ ,  ,  ,  the, t...|(100000,[0,1,2,3,...|\n",
       " beec.txt| \r\n",
       "    A bookplat...|[, , , , , , , a,...|[ ,  ,  ,  ,  ,  ...|(100000,[0,1,2,3,...|\n",
       " blue.txt| \r\n",
       " THE BLUE GRAS...|[, , , , the, blu...|[ ,  ,  ,  the, t...|(100000,[0,1,2,3,...|\n",
       " bost.txt| \r\n",
       " THE  BOSTON C...|[, , , , the, , b...|[ ,  ,  ,  the, t...|(100000,[0,1,2,3,...|\n",
       " brkf.txt| \r\n",
       " Breakfast, Lu...|[, , , , breakfas...|[ ,  ,  ,  breakf...|(100000,[0,1,2,3,...|\n",
       " buck.txt| \r\n",
       " Practical  Ho...|[, , , , practica...|[ ,  ,  ,  practi...|(100000,[0,1,2,3,...|\n",
       " cclu.txt| \r\n",
       "   Cooking in ...|[, , , , , , cook...|[ ,  ,  ,  ,  ,  ...|(100000,[0,1,2,3,...|\n",
       " chas.txt| \r\n",
       " Dr. Chase&#39;s R...|[, , , , dr., cha...|[ ,  ,  ,  dr., d...|(100000,[0,1,2,3,...|\n",
       " chin.txt| \r\n",
       " Chinese-Japan...|[, , , , chinese-...|[ ,  ,  ,  chines...|(100000,[0,1,2,3,...|\n",
       " choc.txt| \r\n",
       " Chocolate and...|[, , , , chocolat...|[ ,  ,  ,  chocol...|(100000,[0,1,2,3,...|\n",
       " comm.txt| \r\n",
       " Common Sense ...|[, , , , common, ...|[ ,  ,  ,  common...|(100000,[0,1,2,3,...|\n",
       " conf.txt| \r\n",
       " The Complete ...|[, , , , the, com...|[ ,  ,  ,  the, t...|(100000,[0,1,2,3,...|\n",
       " coow.txt| \r\n",
       " The Cook&#39;s Ow...|[, , , , the, coo...|[ ,  ,  ,  the, t...|(100000,[0,1,2,3,...|\n",
       " creo.txt| \r\n",
       " La Cuisine Cr...|[, , , , la, cuis...|[ ,  ,  ,  la, la...|(100000,[0,1,2,3,...|\n",
       " dcvb.txt| \r\n",
       " Directions fo...|[, , , , directio...|[ ,  ,  ,  direct...|(100000,[0,1,2,3,...|\n",
       " dish.txt| \r\n",
       "  Dishes &amp;amp;...|[, , , , , dishes...|[ ,  ,  ,  ,  dis...|(100000,[0,1,2,3,...|\n",
       "+---------+--------------------+--------------------+--------------------+--------------------+\n",
       "only showing top 20 rows\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Displaying the result\n",
    "data_select.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+---------+--------------------+--------------------+--------------------+--------------------+\n",
       "doc_title|         doc_content|           doc_words|              ngrams|            features|\n",
       "+---------+--------------------+--------------------+--------------------+--------------------+\n",
       " amem.txt| \n",
       " The American M...|[, , , the, ameri...|[ ,  ,  the, the ...|(100000,[0,4,21,2...|\n",
       " amwh.txt| \r\n",
       "  The American...|[, , , , , the, a...|[ ,  ,  ,  ,  the...|(100000,[0,1,2,3,...|\n",
       " army.txt| \r\n",
       " Manual For Ar...|[, , , , manual, ...|[ ,  ,  ,  manual...|(100000,[0,1,2,3,...|\n",
       " aunt.txt| \r\n",
       "  &#34;Aunt Babett...|[, , , , , &#34;aunt,...|[ ,  ,  ,  ,  &#34;au...|(100000,[0,1,2,3,...|\n",
       " bart.txt| \r\n",
       " THE \r\n",
       "IDEAL B...|[, , , , the, , ,...|[ ,  ,  ,  the, t...|(100000,[0,1,2,3,...|\n",
       " beec.txt| \r\n",
       "    A bookplat...|[, , , , , , , a,...|[ ,  ,  ,  ,  ,  ...|(100000,[0,1,2,3,...|\n",
       " blue.txt| \r\n",
       " THE BLUE GRAS...|[, , , , the, blu...|[ ,  ,  ,  the, t...|(100000,[0,1,2,3,...|\n",
       " bost.txt| \r\n",
       " THE  BOSTON C...|[, , , , the, , b...|[ ,  ,  ,  the, t...|(100000,[0,1,2,3,...|\n",
       " brkf.txt| \r\n",
       " Breakfast, Lu...|[, , , , breakfas...|[ ,  ,  ,  breakf...|(100000,[0,1,2,3,...|\n",
       " buck.txt| \r\n",
       " Practical  Ho...|[, , , , practica...|[ ,  ,  ,  practi...|(100000,[0,1,2,3,...|\n",
       " cclu.txt| \r\n",
       "   Cooking in ...|[, , , , , , cook...|[ ,  ,  ,  ,  ,  ...|(100000,[0,1,2,3,...|\n",
       " chas.txt| \r\n",
       " Dr. Chase&#39;s R...|[, , , , dr., cha...|[ ,  ,  ,  dr., d...|(100000,[0,1,2,3,...|\n",
       " chin.txt| \r\n",
       " Chinese-Japan...|[, , , , chinese-...|[ ,  ,  ,  chines...|(100000,[0,1,2,3,...|\n",
       " choc.txt| \r\n",
       " Chocolate and...|[, , , , chocolat...|[ ,  ,  ,  chocol...|(100000,[0,1,2,3,...|\n",
       " comm.txt| \r\n",
       " Common Sense ...|[, , , , common, ...|[ ,  ,  ,  common...|(100000,[0,1,2,3,...|\n",
       " conf.txt| \r\n",
       " The Complete ...|[, , , , the, com...|[ ,  ,  ,  the, t...|(100000,[0,1,2,3,...|\n",
       " coow.txt| \r\n",
       " The Cook&#39;s Ow...|[, , , , the, coo...|[ ,  ,  ,  the, t...|(100000,[0,1,2,3,...|\n",
       " creo.txt| \r\n",
       " La Cuisine Cr...|[, , , , la, cuis...|[ ,  ,  ,  la, la...|(100000,[0,1,2,3,...|\n",
       " dcvb.txt| \r\n",
       " Directions fo...|[, , , , directio...|[ ,  ,  ,  direct...|(100000,[0,1,2,3,...|\n",
       " dish.txt| \r\n",
       "  Dishes &amp;amp;...|[, , , , , dishes...|[ ,  ,  ,  ,  dis...|(100000,[0,1,2,3,...|\n",
       "+---------+--------------------+--------------------+--------------------+--------------------+\n",
       "only showing top 20 rows\n",
       "\n",
       "Total Files -  76\n",
       "Column Data types  [(&#39;doc_title&#39;, &#39;string&#39;), (&#39;doc_content&#39;, &#39;string&#39;), (&#39;doc_words&#39;, &#39;array&lt;string&gt;&#39;), (&#39;ngrams&#39;, &#39;array&lt;string&gt;&#39;), (&#39;features&#39;, &#39;vector&#39;)]\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Used MinHashLSH fucntion from SparkMLand using this to generate the hashes which are generated at a grouped level.\n",
    "# We are taking line wise split grouped by the no of words mentioned in the Vocab size\n",
    "# This gives us the flexibility of checking how many rows were similar.\n",
    "#LSH class for Jaccard distance.\n",
    "min_hash_lsh = MinHashLSH(inputCol=\"features\", outputCol=\"hashValues\", seed=12345).setNumHashTables(3)\n",
    "minhash_model = min_hash_lsh.fit(data_select)\n",
    "minhash_model.transform(data_select)\n",
    "data_select.show()\n",
    "print(\"Total Files - \",data_select.count())\n",
    "print(\"Column Data types \",data_select.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Approximate similarity join takes two datasets (data_select in this case) and approximately returns pairs of rows in the datasets whose distance is smaller than a user-defined threshold (0.7 in this case).\n",
    "approx_model = minhash_model.approxSimilarityJoin(data_select, data_select,3.0, distCol=\"JaccardDistance\").select(col(\"datasetA.doc_title\").alias(\"Title A\"), col(\"datasetB.doc_title\").alias(\"Title B\"),col(\"JaccardDistance\")).sort(desc(\"JaccardDistance\")).dropDuplicates(['JaccardDistance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+--------+--------+------------------+\n",
       " Title A| Title B|   JaccardDistance|\n",
       "+--------+--------+------------------+\n",
       "frca.txt|zuni.txt|0.9060486311367439|\n",
       "hosf.txt|youn.txt|0.9167024843118714|\n",
       "engl.txt|youn.txt|0.9174130174584176|\n",
       "chin.txt|mara.txt|0.9199712969257496|\n",
       "epib.txt|youn.txt| 0.895732126353705|\n",
       "chin.txt|linc.txt|0.8978748860528715|\n",
       "chin.txt|scie.txt|0.9045326682713221|\n",
       "chin.txt|comm.txt|0.8836166440713318|\n",
       "blue.txt|mara.txt|0.8905488186023253|\n",
       "hote.txt|bost.txt|0.8946768848384424|\n",
       "+--------+--------+------------------+\n",
       "only showing top 10 rows\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Displaying only top 10 rows because it was taking alot of computational time for the entire data. \n",
    "approx_model.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+--------+--------+------------------+\n",
       " Title A| Title B|   JaccardDistance|\n",
       "+--------+--------+------------------+\n",
       "frca.txt|zuni.txt|0.9060486311367439|\n",
       "hosf.txt|youn.txt|0.9167024843118714|\n",
       "engl.txt|youn.txt|0.9174130174584176|\n",
       "chin.txt|mara.txt|0.9199712969257496|\n",
       "epib.txt|youn.txt| 0.895732126353705|\n",
       "+--------+--------+------------------+\n",
       "only showing top 5 rows\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#setting a threshold 0.7 to decide take a decidsion based on JaccardDistance\n",
    "#I am displaying only top 5 rows as it was taking alot of computational time for the entire dataset. \n",
    "matches = approx_model.filter(approx_model['JaccardDistance'] > 0.7)\n",
    "matches.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "name": "Big Data Q5",
  "notebookId": 1130610787585566
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
